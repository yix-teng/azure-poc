{
	"name": "SPARK_SQL_Runner_Script",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"* This script will run the TPC SPARK SQL runner script to test all 99 queries.\n",
					"* Make sure the current spark cluster is selected and that the database and ADLS output location is within your resource group\n",
					"* Make sure to have the *interations = 3* (for 3 runs)\n",
					"* The 2nd script will load the resultant json files and populate the timings in a nice table. This output should go into the confluence page."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
					" \n",
					"// Note: Declare \"sqlContext\" for Spark 2.x version\n",
					"val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
					" \n",
					"val tpcds = new TPCDS (sqlContext = sqlContext)\n",
					"// Set:\n",
					"val databaseName = \"sparkdb\" // name of database with TPCDS data.\n",
					"sql(s\"use $databaseName\")\n",
					"val resultLocation = \"abfss://tpcdata@scbpocdev.dfs.core.windows.net/tpcds_results\" // place to write results\n",
					"val iterations = 3 // how many iterations of queries to run.\n",
					"val queries = tpcds.tpcds2_4Queries // queries to run.\n",
					"val timeout = 24*60*60 // timeout, in seconds.\n",
					"// Run:\n",
					"val experiment = tpcds.runExperiment(\n",
					"  queries, \n",
					"  iterations = iterations,\n",
					"  resultLocation = resultLocation,\n",
					"  forkThread = true)\n",
					"experiment.waitForFinish(timeout)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import org.apache.spark.sql.functions._\n",
					"\n",
					"val resultLocation = \"abfss://tpcdata@scbpocdev.dfs.core.windows.net/tpcds_results\" // place to write results\n",
					"val result = spark.read.json(resultLocation).filter(\"timestamp = 1631102177772\").select(explode($\"results\").as(\"r\"))\n",
					"result.createOrReplaceTempView(\"result\") \n",
					"spark.sql(\"select substring(r.name,1,100) as Name, bround((r.parsingTime+r.analysisTime+r.optimizationTime+r.planningTime+r.executionTime)/1000.0,1) as Runtime_sec  from result\").show(150)"
				],
				"execution_count": null
			}
		]
	}
}