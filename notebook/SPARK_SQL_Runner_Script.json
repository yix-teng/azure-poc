{
	"name": "SPARK_SQL_Runner_Script",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "previewtest",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 5,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "5",
				"spark.dynamicAllocation.maxExecutors": "5"
			}
		},
		"metadata": {
			"saveOutput": true,
			"kernelspec": {
				"name": "synapse_spark",
				"display_name": "Synapse Spark"
			},
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/dedfa758-9fb0-4c1b-b0e2-25c590d4f804/resourceGroups/SCB-POC-DEV/providers/Microsoft.Synapse/workspaces/sqlservlessinternalselfmanaged/bigDataPools/previewtest",
				"name": "previewtest",
				"type": "Spark",
				"endpoint": "https://sqlservlessinternalselfmanaged.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/previewtest",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"* This script will run the TPC SPARK SQL runner script to test all 99 queries.\n",
					"* Make sure the current spark cluster is selected and that the database and ADLS output location is within your resource group\n",
					"* Make sure to have the *interations = 3* (for 3 runs)\n",
					"* The 2nd script will load the resultant json files and populate the timings in a nice table. This output should go into the confluence page."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
					" \n",
					"// Note: Declare \"sqlContext\" for Spark 2.x version\n",
					"val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
					" \n",
					"val tpcds = new TPCDS (sqlContext = sqlContext)\n",
					"// Set:\n",
					"val databaseName = \"sparkdb\" // name of database with TPCDS data.\n",
					"sql(s\"use $databaseName\")\n",
					"val resultLocation = \"abfss://tpcdata@scbpocdev.dfs.core.windows.net/tpcds_results\" // place to write results\n",
					"val iterations = 3 // how many iterations of queries to run.\n",
					"val queries = tpcds.tpcds2_4Queries // queries to run.\n",
					"val timeout = 24*60*60 // timeout, in seconds.\n",
					"// Run:\n",
					"val experiment = tpcds.runExperiment(\n",
					"  queries, \n",
					"  iterations = iterations,\n",
					"  resultLocation = resultLocation,\n",
					"  forkThread = true)\n",
					"experiment.waitForFinish(timeout)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					"import org.apache.spark.sql.functions._\n",
					"\n",
					"val resultLocation = \"abfss://tpcdata@scbpocdev.dfs.core.windows.net/tpcds_results\" // place to write results\n",
					"val result = spark.read.json(resultLocation).filter(\"timestamp = 1631102177772\").select(explode($\"results\").as(\"r\"))\n",
					"result.createOrReplaceTempView(\"result\") \n",
					"spark.sql(\"select substring(r.name,1,100) as Name, bround((r.parsingTime+r.analysisTime+r.optimizationTime+r.planningTime+r.executionTime)/1000.0,1) as Runtime_sec  from result\").show(150)"
				],
				"execution_count": 4
			}
		]
	}
}