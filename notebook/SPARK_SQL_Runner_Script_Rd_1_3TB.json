{
	"name": "SPARK_SQL_Runner_Script_Rd_1_3TB",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "Rd1Cluster",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"kernelspec": {
				"name": "synapse_spark",
				"display_name": "Synapse Spark"
			},
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/dedfa758-9fb0-4c1b-b0e2-25c590d4f804/resourceGroups/POC-SQL_ADLS-ENV/providers/Microsoft.Synapse/workspaces/pocsqladlsenv/bigDataPools/Rd1Cluster",
				"name": "Rd1Cluster",
				"type": "Spark",
				"endpoint": "https://pocsqladlsenv.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/Rd1Cluster",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 4,
				"cores": 4,
				"memory": 28
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"* This script will run the TPC SPARK SQL runner script to test all 99 queries.\n",
					"* Make sure the current spark cluster is selected and that the database and ADLS output location is within your resource group\n",
					"* Make sure to have the *interations = 3* (for 3 runs)\n",
					"* The 2nd script will load the resultant json files and populate the timings in a nice table. This output should go into the confluence page."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
					" \n",
					"// Note: Declare \"sqlContext\" for Spark 2.x version\n",
					"val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
					" \n",
					"val tpcds = new TPCDS (sqlContext = sqlContext)\n",
					"// Set:\n",
					"val databaseName = \"Round_1_create_external_spark_tables_3TB_DB\" // name of database with TPCDS data.\n",
					"sql(s\"use $databaseName\")\n",
					"val resultLocation = \"abfss://dbstorage3000@scbpocdev.dfs.core.windows.net/synpase_tpcds_results\" // place to write results\n",
					"val iterations = 3 // how many iterations of queries to run.\n",
					"val queries = tpcds.tpcds2_4Queries // queries to run.\n",
					"val timeout = 24*60*60 // timeout, in seconds.\n",
					"// Run:\n",
					"val experiment = tpcds.runExperiment(\n",
					"  queries, \n",
					"  iterations = iterations,\n",
					"  resultLocation = resultLocation,\n",
					"  forkThread = true)\n",
					"experiment.waitForFinish(timeout)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import org.apache.spark.sql.functions._\n",
					"\n",
					"val resultLocation = \"abfss://dbstorage3000@scbpocdev.dfs.core.windows.net/synpase_tpcds_results\" // place to write results\n",
					"val result = spark.read.json(resultLocation).filter(\"timestamp = 1631102177772\").select(explode($\"results\").as(\"r\"))\n",
					"result.createOrReplaceTempView(\"result\") \n",
					"spark.sql(\"select substring(r.name,1,100) as Name, bround((r.parsingTime+r.analysisTime+r.optimizationTime+r.planningTime+r.executionTime)/1000.0,1) as Runtime_sec  from result\").show(150)"
				],
				"execution_count": null
			}
		]
	}
}