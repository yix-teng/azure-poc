{
	"name": "SPARK_SQL_Runner_Script_Rd_2_3TB",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SparkRd2WL",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 4,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "4",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "e4b2db15-b2fd-47c4-ad18-eb86502ae8d1"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_spark",
				"display_name": "Synapse Spark"
			},
			"language_info": {
				"name": "scala"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/dedfa758-9fb0-4c1b-b0e2-25c590d4f804/resourceGroups/POC-Spark-Env/providers/Microsoft.Synapse/workspaces/pocsparkenv/bigDataPools/SparkRd2WL",
				"name": "SparkRd2WL",
				"type": "Spark",
				"endpoint": "https://pocsparkenv.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkRd2WL",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.1",
				"nodeCount": 5,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"* This script will run the TPC SPARK SQL runner script to test all 99 queries.\n",
					"* Make sure the current spark cluster is selected and that the database and ADLS output location is within your resource group\n",
					"* Make sure to have the *interations = 3* (for 3 runs)\n",
					"* The 2nd script will load the resultant json files and populate the timings in a nice table. This output should go into the confluence page."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import com.databricks.spark.sql.perf.tpcds.TPCDSTables\n",
					"\n",
					"\n",
					"val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
					"\n",
					"val databaseName = \"External_spark_tables_3TB_DB\" // name of database with TPCDS data.\n",
					"val scaleFactor = \"1000\" // scaleFactor defines the size of the dataset to generate (in GB).\n",
					"val format = \"parquet\" // valid spark format like parquet \"parquet\".\n",
					"\n",
					"val tables = new TPCDSTables(sqlContext,\n",
					"    dsdgenDir = \"/tmp/tpcds-kit/tools\", // location of dsdgen\n",
					"    scaleFactor = scaleFactor,\n",
					"    useDoubleForDecimal = false, // true to replace DecimalType with DoubleType\n",
					"    useStringForDate = false) // true to replace DateType with StringType\n",
					" \n",
					"// Set:\n",
					"sql(s\"use $databaseName\")\n",
					" \n",
					"// For CBO only, gather statistics on all columns:\n",
					"tables.analyzeTables(databaseName, analyzeColumns = true)  "
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"import com.databricks.spark.sql.perf.tpcds.TPCDS\n",
					" \n",
					"// Note: Declare \"sqlContext\" for Spark 2.x version\n",
					"val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
					" \n",
					"val tpcds = new TPCDS (sqlContext = sqlContext)\n",
					"// Set:\n",
					"val databaseName = \"External_spark_tables_3TB_DB\" // name of database with TPCDS data.\n",
					"sql(s\"use $databaseName\")\n",
					"val resultLocation = \"abfss://dbstorage3000@scbpocdev.dfs.core.windows.net/rd2_3_tb_synpase_tpcds_results_wl\" // place to write results\n",
					"val iterations = 3 // how many iterations of queries to run.\n",
					"val queries = tpcds.tpcds2_4Queries // queries to run.\n",
					"val timeout = 24*60*60 // timeout, in seconds.\n",
					"// Run:\n",
					"val experiment = tpcds.runExperiment(\n",
					"  queries, \n",
					"  iterations = iterations,\n",
					"  resultLocation = resultLocation,\n",
					"  forkThread = true)\n",
					"experiment.waitForFinish(timeout)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import org.apache.spark.sql.functions._\n",
					"\n",
					"val resultLocation = \"abfss://dbstorage3000@scbpocdev.dfs.core.windows.net/rd2_3_tb_synpase_tpcds_results\" // place to write results\n",
					"val result = spark.read.json(resultLocation).filter(\"timestamp = 1631102177772\").select(explode($\"results\").as(\"r\"))\n",
					"result.createOrReplaceTempView(\"result\") \n",
					"spark.sql(\"select substring(r.name,1,100) as Name, bround((r.parsingTime+r.analysisTime+r.optimizationTime+r.planningTime+r.executionTime)/1000.0,1) as Runtime_sec  from result\").show(150)"
				],
				"execution_count": null
			}
		]
	}
}